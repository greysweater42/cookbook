---
title: "decision trees"
date: 2019-01-11T21:18:57+01:00
draft: false
categories: ["Machine learning", "R"]
---



<div id="what-are-decision-trees-and-why-would-you-use-them" class="section level2">
<h2>1. What are decision trees and why would you use them?</h2>
<ul>
<li><p>decision trees are among the most popular classification algorithms;</p></li>
<li><p>they divide the dataset hierarchically starting from the full dataset, until the stop criterium is met, e.g. the minimum size of a leaf and the purity of leaf;</p></li>
<li><p>in general they are easy to understand, interpret and visualise</p></li>
<li><p>however they are not very efficient, but they can scale, i.e. they can be processed in parallel;</p></li>
<li><p>understanding decision trees is important if you want to learn <a href="http://tomis9.com/random-forests">random forests</a>.</p></li>
</ul>
</div>
<div id="a-few-hello-world-examples" class="section level2">
<h2>2. A few “hello world” examples</h2>
<div id="rpart" class="section level3">
<h3>rpart</h3>
<p>Calculating decision trees in R is rather straightforward. Probably the best known package which supports decision trees is <code>rpart</code>. Let’s have a look at a short example:</p>
<pre class="r"><code>library(rpart)
tree &lt;- rpart(Species ~ ., iris)

print(tree)</code></pre>
<pre><code>## n= 150 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 150 100 setosa (0.33333 0.33333 0.33333)  
##   2) Petal.Length&lt; 2.45 50   0 setosa (1.00000 0.00000 0.00000) *
##   3) Petal.Length&gt;=2.45 100  50 versicolor (0.00000 0.50000 0.50000)  
##     6) Petal.Width&lt; 1.75 54   5 versicolor (0.00000 0.90741 0.09259) *
##     7) Petal.Width&gt;=1.75 46   1 virginica (0.00000 0.02174 0.97826) *</code></pre>
<p>The summary above informs us, that the root (the whole dataset) was divided into two subsets, based on the value of the attribute <em>Petal.Length</em>. According to the docs (<code>?print.rpart</code>)</p>
<blockquote>
<p>Information for each node includes the node number, split, size, deviance, and fitted value.</p>
</blockquote>
<p>In our case, the root is classified as <em>setosa</em>, where among 150 observations 100 is classified incorrectly and there is 33% of setosa, versicolor and virginica each in this dataset.</p>
<p>Probably you will also be interested in the plot method:</p>
<pre class="r"><code>plot(tree, margin = 0.1)
text(tree, use.n = TRUE, cex = 0.75)</code></pre>
<p><img src="/decision_trees_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>which produces a rather modest view of our tree. Luckily the are various packages, which can make the plot look more neat:</p>
<pre class="r"><code>library(rpart.plot)
rpart.plot(tree)</code></pre>
<p><img src="/decision_trees_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="decisiontree" class="section level3">
<h3>decisionTree</h3>
<p>If you want to go really fancy, you can use my <code>decisionTree</code> package, which grows a decision tree for a binary response and has a legible plot method. The package is available at github, so you can download it easily with:</p>
<pre class="r"><code>devtools::install_github(&#39;tomis9/decisionTree&#39;)</code></pre>
<p>First, let’s create a sample dataset based on iris dataset, which contains a binary response.</p>
<pre class="r"><code>d &lt;- iris[, c(&quot;Species&quot;, &quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)]
d$Species &lt;- as.character(d$Species)
d$Species[d$Species != &quot;setosa&quot;] &lt;- &quot;non-setosa&quot;
x &lt;- d$Sepal.Length
x[d$Sepal.Length &lt;= 5.2] &lt;- &quot;Very Short&quot;
x[d$Sepal.Length &gt;  5.2 &amp; d$Sepal.Length &lt;= 6.1] &lt;- &quot;Short&quot;
x[d$Sepal.Length &gt;  6.1 &amp; d$Sepal.Length &lt;= 7.0] &lt;- &quot;Long&quot;
x[d$Sepal.Length &gt;  7.0] &lt;- &quot;Very Long&quot;
d$Sepal.Length &lt;- x

summary(d)</code></pre>
<pre><code>##    Species          Sepal.Length        Sepal.Width  
##  Length:150         Length:150         Min.   :2.00  
##  Class :character   Class :character   1st Qu.:2.80  
##  Mode  :character   Mode  :character   Median :3.00  
##                                        Mean   :3.06  
##                                        3rd Qu.:3.30  
##                                        Max.   :4.40</code></pre>
<p>As you can see, the response variable is <code>Species</code>, which is binary. The other two values are independent; one of them is categorical, the other - continuous.</p>
<pre class="r"><code>library(decisionTree)
tree &lt;- decisionTree(d, eta = 5, purity=0.95, minsplit=0)
plot(tree)</code></pre>
<p><img src="/decision_trees_files/figure-html/unnamed-chunk-6-1.png" width="960" /></p>
</div>
</div>
<div id="interesting-links" class="section level2">
<h2>3. Interesting links</h2>
<ul>
<li><p><a href="https://www.guru99.com/r-decision-trees.html">Decision Trees in R with Example - pretty much the same thing as the tutorial above (rpart)</a></p></li>
<li><p><a href="https://www.datacamp.com/community/tutorials/decision-trees-R">datacamp’s article on classification and regression trees, bagging, random forests, boosting, …</a></p></li>
</ul>
</div>
