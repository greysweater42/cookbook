---
title: "useful processing"
date: 2019-05-17T15:44:38+02:00
draft: false
categories: ["Machine learning", "Data engineering"]
tags: []
---



<div id="what-is-useful-processing" class="section level2">
<h2>1. What is useful processing?</h2>
<ul>
<li>many machine learning algorithms require the same kinds of data preprocessing in order for them to work properly. In other words, theses kinds of processing are useful.</li>
</ul>
</div>
<div id="examples" class="section level2">
<h2>2. Examples</h2>
<div id="one-hot-encoding" class="section level3">
<h3>one-hot encoding</h3>
<p><em>R</em></p>
<pre class="r"><code># data.table
dt_iris &lt;- data.table::as.data.table(iris)
mltools::one_hot(dt_iris)</code></pre>
<pre><code>##      Sepal.Length Sepal.Width Petal.Length Petal.Width Species_setosa
##   1:          5.1         3.5          1.4         0.2              1
##   2:          4.9         3.0          1.4         0.2              1
##   3:          4.7         3.2          1.3         0.2              1
##   4:          4.6         3.1          1.5         0.2              1
##   5:          5.0         3.6          1.4         0.2              1
##  ---                                                                 
## 146:          6.7         3.0          5.2         2.3              0
## 147:          6.3         2.5          5.0         1.9              0
## 148:          6.5         3.0          5.2         2.0              0
## 149:          6.2         3.4          5.4         2.3              0
## 150:          5.9         3.0          5.1         1.8              0
##      Species_versicolor Species_virginica
##   1:                  0                 0
##   2:                  0                 0
##   3:                  0                 0
##   4:                  0                 0
##   5:                  0                 0
##  ---                                     
## 146:                  0                 1
## 147:                  0                 1
## 148:                  0                 1
## 149:                  0                 1
## 150:                  0                 1</code></pre>
<pre class="r"><code># caret
library(caret)
dummy &lt;- caret::dummyVars(&quot; ~ .&quot;, data = iris)
head(predict(dummy, iris))</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species.setosa
## 1          5.1         3.5          1.4         0.2              1
## 2          4.9         3.0          1.4         0.2              1
## 3          4.7         3.2          1.3         0.2              1
## 4          4.6         3.1          1.5         0.2              1
## 5          5.0         3.6          1.4         0.2              1
## 6          5.4         3.9          1.7         0.4              1
##   Species.versicolor Species.virginica
## 1                  0                 0
## 2                  0                 0
## 3                  0                 0
## 4                  0                 0
## 5                  0                 0
## 6                  0                 0</code></pre>
<pre class="r"><code># dplyr is not that clever
library(dplyr)
iris %&gt;%
  mutate(&quot;Species_setosa&quot; = ifelse(Species == &quot;setosa&quot;, 1, 0)) %&gt;%
  mutate(&quot;Species_virgninica&quot; = ifelse(Species == &quot;virgninica&quot;, 1, 0)) %&gt;%
  mutate(&quot;Species_versicolor&quot; = ifelse(Species == &quot;versicolor&quot;, 1, 0)) %&gt;%
  head()</code></pre>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Species_setosa
## 1          5.1         3.5          1.4         0.2  setosa              1
## 2          4.9         3.0          1.4         0.2  setosa              1
## 3          4.7         3.2          1.3         0.2  setosa              1
## 4          4.6         3.1          1.5         0.2  setosa              1
## 5          5.0         3.6          1.4         0.2  setosa              1
## 6          5.4         3.9          1.7         0.4  setosa              1
##   Species_virgninica Species_versicolor
## 1                  0                  0
## 2                  0                  0
## 3                  0                  0
## 4                  0                  0
## 5                  0                  0
## 6                  0                  0</code></pre>
<p>As you can see, <code>caret</code> recognised dummy variables (<code>Species</code>) and processed them to binary variables.</p>
<p>TODO: <code>library(dummies); library(onehot)</code></p>
<p><em>Python</em></p>
<pre class="python"><code># https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn import datasets
data = datasets.load_iris()
y = [data.target_names[i] for i in data.target]
# integer encode
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(y)
print(integer_encoded[:5])
# binary encode</code></pre>
<pre><code>## [0 0 0 0 0]</code></pre>
<pre class="python"><code>onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
print(onehot_encoded[:5])</code></pre>
<pre><code>## [[1. 0. 0.]
##  [1. 0. 0.]
##  [1. 0. 0.]
##  [1. 0. 0.]
##  [1. 0. 0.]]</code></pre>
</div>
<div id="scaling" class="section level3">
<h3>scaling</h3>
<p>In R it’s extremely simple. <em>base R</em></p>
<pre class="r"><code>scale(1:5)</code></pre>
<pre><code>##         [,1]
## [1,] -1.2649
## [2,] -0.6325
## [3,]  0.0000
## [4,]  0.6325
## [5,]  1.2649
## attr(,&quot;scaled:center&quot;)
## [1] 3
## attr(,&quot;scaled:scale&quot;)
## [1] 1.581</code></pre>
<pre class="r"><code>mean(1:5)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="r"><code>sd(1:5)</code></pre>
<pre><code>## [1] 1.581</code></pre>
<pre class="r"><code>sc &lt;- scale(iris[,1:4])
head(sc)</code></pre>
<pre><code>##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]      -0.8977     1.01560       -1.336      -1.311
## [2,]      -1.1392    -0.13154       -1.336      -1.311
## [3,]      -1.3807     0.32732       -1.392      -1.311
## [4,]      -1.5015     0.09789       -1.279      -1.311
## [5,]      -1.0184     1.24503       -1.336      -1.311
## [6,]      -0.5354     1.93331       -1.166      -1.049</code></pre>
<pre class="r"><code>attributes(sc)</code></pre>
<pre><code>## $dim
## [1] 150   4
## 
## $dimnames
## $dimnames[[1]]
## NULL
## 
## $dimnames[[2]]
## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot;  &quot;Petal.Length&quot; &quot;Petal.Width&quot; 
## 
## 
## $`scaled:center`
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##        5.843        3.057        3.758        1.199 
## 
## $`scaled:scale`
## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##       0.8281       0.4359       1.7653       0.7622</code></pre>
<p><em>Python</em></p>
<pre class="python"><code>from sklearn.preprocessing import scale, StandardScaler
from sklearn import datasets
import numpy as np
# simple approach
sc = scale(np.arange(1, 6))</code></pre>
<pre><code>## /usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.
##   warnings.warn(msg, DataConversionWarning)</code></pre>
<pre class="python"><code>print(np.std(sc))</code></pre>
<pre><code>## 0.9999999999999999</code></pre>
<pre class="python"><code>print(np.mean(sc))
# and a way compatible with pandas</code></pre>
<pre><code>## 0.0</code></pre>
<pre class="python"><code>data = datasets.load_iris()
X, y = data.data, data.target
scaler = StandardScaler()
scaled_df = scaler.fit_transform(X)
print(scaled_df.mean(axis=0))</code></pre>
<pre><code>## [-1.69031455e-15 -1.63702385e-15 -1.48251781e-15 -1.62314606e-15]</code></pre>
<pre class="python"><code>print(scaled_df.std(axis=0))</code></pre>
<pre><code>## [1. 1. 1. 1.]</code></pre>
</div>
<div id="splitting-your-dataset-into-train-and-test-subsets" class="section level3">
<h3>splitting your dataset into train and test subsets</h3>
<p>The idea for the following solution comes from <a href="https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function">this post at stackoverflow</a>.</p>
<p><em>base R</em></p>
<pre class="r"><code>train_test_split &lt;- function(test_proportion = 0.75, dataset) {
    smp_size &lt;- floor(test_proportion * nrow(dataset))
    train_ind &lt;- sample(seq_len(nrow(dataset)), size = smp_size)

    train &lt;- dataset[train_ind, ]
    test &lt;- dataset[-train_ind, ]
    return(list(train = train, test = test))
}
library(gsubfn)
list[train, test] &lt;- train_test_split(0.8, iris)</code></pre>
<p><em>caret R</em></p>
<pre class="r"><code>library(caret)
# ..., the random sampling is done within the
# levels of ‘y’ when ‘y’ is a factor in an attempt to balance the class
# distributions within the splits.
# I provide package&#39;s name before function&#39;s name for clarity
trainIndex &lt;- caret::createDataPartition(iris$Species, p=0.7, list = FALSE, 
                                         times = 1)
train &lt;- iris[trainIndex,]
test &lt;- iris[-trainIndex,]</code></pre>
<p><em>Python - sklearn</em></p>
<pre class="python"><code>from sklearn import datasets
from sklearn.model_selection import train_test_split
data = datasets.load_iris()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)</code></pre>
<p><em>Python - pandas</em></p>
<pre class="python"><code>import pandas as pd
# data = pd.DataFrame(data)
# train = data.sample(frac=0.8)
# test = data.drop(train.index)</code></pre>
</div>
<div id="sklearn-pipeline" class="section level3">
<h3>sklearn pipeline</h3>
<ul>
<li><a href="https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976">a short article about sklearn pipelines</a></li>
</ul>
<pre class="python"><code>from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn import datasets
data = datasets.load_iris()
X, y = data.data, data.target
# one way
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
svc = SVC()
svc.fit(X_scaled, y)
# or another - with pipeline
from sklearn.pipeline import Pipeline
svc = Pipeline([(&#39;scaler&#39;, StandardScaler()), (&#39;SVM&#39;, SVC())])
svc.fit(X, y)</code></pre>
<p>TODO: <code>pd.get_dummies()</code></p>
</div>
</div>
